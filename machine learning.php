<!DOCTYPE html>
<a href="https://icons8.com/icon/wFfu6zXx15Yk/home"></a>
<script defer src="https://friconix.com/cdn/friconix.js"> </script>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Engineers Eden - MACHINE LEARNING Blog</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/business-casual.css" rel="stylesheet">

    <!-- Fonts -->
    <link href="http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Josefin+Slab:100,300,400,600,700,100italic,300italic,400italic,600italic,700italic" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <div class="brand"><img src="https://img.icons8.com/cute-clipart/64/000000/machine-learning.png"/> Engineers Eden</div>
    <div class="address-bar">Banglore | Karnataka | India <img src="https://img.icons8.com/cute-clipart/64/000000/india.png"/></div>

    <!-- Navigation -->
    <?php require_once 'nav.php'; ?>

    <div class="container">

        <div class="row">
            <div class="box">
            <h2 class="text-center">Welcome <?php echo " "; echo "' "; echo $fname; echo " '"; ?> - <a href="logout.php">Logout</a></h2>
				
                    <hr>
                    <h2 class="intro-text text-center">Engineers Eden -<br> 
                        <strong> MACHINE LEARNING SECTION</strong>
                    </h2>
                    <hr>
                
             <div class="col-lg-4"><hr></div>	
                <div class="col-lg-11">
                    <h3>100% FREE UDEMY COUPONS - </h3>
                    <h3><p>1-<a href="https://www.udemy.com/course/machine-learning-for-researchers/?couponCode=6E0F34FAE9A5DF407CE3">Machine Learning For Researchers
</a></p>
                        <small>july 2, 2020</small>
                    </h3>
                   
                <div class="col-lg-4"><hr></div>	


             <div class="col-lg-12 text-center">
                    <img class="img-responsive img-border img-full" src="img/dl1.jpg" alt="">
                    <h2> Pytorch 101
                        <br>
                        <small>july 02, 2020</small>
                    </h2>
                    <p>An Hands on Introduction to Deep Learning</p>
                    <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#myModal504">Read More</button>
                    <hr>
                </div>
             <div class="col-lg-12 text-center">
                    <img class="img-responsive img-border img-full" src="img/nn1.jpg" alt="">
                    <h2> Understanding Neural Networks
                        <br>
                        <small>june 30, 2020</small>
                    </h2>
                    <p>We Explore How Neural Networks Function in Order to Build an Intuitive Understanding of Deep Learning</p>
                    <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#myModal503">Read More</button>
                    <hr>
                </div>
			 <div class="col-lg-12 text-center">
                    <img class="img-responsive img-border img-full" src="img/vml.jpg" alt="">
                    <h2> Basic Knowledge Of Machine Learning
                        <br>
                        <small>june 28, 2020</small>
                    </h2>
                    <p>Intrested people in Machine Learning check out this!!!</p>
                    <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#myModal36">Read More</button>
                    <hr>
                </div>
			
			
			
			
			
			
			
			
			
			
<div class="col-lg-12 text-center">
                    <ul class="pager">
                        <li class="previous"><a href="#">&larr; Newer</a>
                        </li>
                        <li class="next"><a href="#">Older &rarr;</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>

    </div>
    <!-- /.container -->
<!-- Modal 504 -->
	<div id="myModal504" class="modal fade" role="dialog">
	  <div class="modal-dialog">

		<!-- Modal content-->
		<div class="modal-content">
		  <div class="modal-header">
			<button type="button" class="close" data-dismiss="modal">&times;</button>
			<h4 class="modal-title">EXPLORING DEEP LEARNING WITH PYTORCH</h4>
		  </div>
		  <div class="modal-body">
<p>Whether you’ve noticed it or not, Deep Learning (DL) plays an important part in all our lives. From the voice assistants and auto-correct services on your smartphone to the automation of large industries, deep learning is the underlying concept behind these meteoric rises in human progress. A major concept that we implement in deep learning is that of neural networks.</p>
<br>
<p><b>What exactly are Neural Networks?</b></p>
<br>
<p>A neural network is a computing algorithm of an interconnected system of mathematical formulae used to make predictions by “training” the algorithm on data relevant to the prediction to be made. This is partly inspired by the way neurons are connected in biological brains. With the ability to make near life-like predictions of numerical, image and character data, given the appropriate amount of training, neural networks(NNs) have become integral to advancements in the field.</p>
<br>
<img class="img-responsive img-border img-full" src="img/dl2.jpg" alt="">
<br>
<p><b>Creating your own neural network with Pytorch</b></p>
<br>
<p>To create our own DL models with neural networks, we have a number of powerful and easy to use open-source frameworks to make use of, none which is as simple and intuitive as Pytorch, a library in the Python Programming Language.</p>
<br>
<p>Pytorch, unlike most other popular frameworks, makes use of dynamic computation, allowing for greater flexibility when building more complex architectures.</p>
<br>
<p>The data in Pytorch are in 2 forms:</p>
<br>
<p>1-Tensors: these are similar to numpy arrays, which can also be used on GPUs to provide increased performance.
<br>
2-Variables: these form a thin wrapper around a tensor object, its gradient and a reference to the function that created it.<br>
Now let’s understand how to use Pytorch by creating our very own XOR gate using Neural Networks.
</p>
<br>
<p><b>Prerequisites:</b><br>
Before we get into it, there are a few prerequisites.<br>
1-A basic-level understanding of the Python programming language<br>
2-Familiarity with Pandas, Numpy and matplotlib<br>
3-A basic understanding of data science terms and practices is helpful, but not compulsory.<br></p>
<br>
<p><b>Importing libraries and metadata:</b><br>
First we import the relevant libraries we’ll use to create our neural network with pytorch.<br>
<img class="img-responsive img-border img-full" src="img/dl3.jpg" alt="">
<br>
Next we import the libraries we’ll use to load and read the csv file which holds the metadata.<br>
<img class="img-responsive img-border img-full" src="img/dl4.jpg" alt="">
<br>
We then import the libraries used for the Data Iterator<br>
<img class="img-responsive img-border img-full" src="img/dl5.jpg" alt="">
<br>
Finally, we import the csv file containing the Metadata used to train our network, using the read_csv() function in pandas.<br>
<img class="img-responsive img-border img-full" src="img/dl6.jpg" alt="">
<br></p>
<p><b>Creating our own Custom Data Iterator using the Dataset class</b><br>
Pytorch allows us the flexibility to create our own custom Dataset object with the help of the Dataset class. The Dataset class provides 2 basic functions for this purpose:<br>
The _len_ () function allows us to obtain information on the size of the dataset and the _getitem_ () which, given an index, can return a sample from the dataset, located at the specified location.<br>
<img class="img-responsive img-border img-full" src="img/dl7.jpg" alt="">
<br>
After defining our class, we can create our class instance and create our torchvision dataset object by loading the dataframe df which contains our metadata which we previously imported using the read_csv() function.<br>
<img class="img-responsive img-border img-full" src="img/dl8.jpg" alt="">
<br></p>
<p><b>Defining our Neural Network:</b><br>
Now it’s finally time to create our neural network! To accomplish this, we create a class XORnet to define our NN.<br>
The neural network we’re gonna create has 1 input layer (2 nodes), 1 hidden layer (3 nodes) and an output layer (2 nodes). We initialize these layers using the _init_() function in our XORnet class. We define the forward() function to define the fully connected network with the ReLU and Sigmoid activation functions for the hidden and output layers respectively.<br>
<h><b>But what do these activation functions do?</b></h><br>
Activation functions are simply mathematical functions that are used to determine the output of a given node/neuron in a Neural network, whether it’s useful or relevant for the predictions made by the given model.<br>
The Sigmoid function is a type of logistic function which outputs a value between 0 and 1, suitable for a Logical gate like the XOR function. The ReLU (Rectified Linear Unit) function, meanwhile, is a common function used in hidden layers, as it can be used to work around the famous “Vanishing Gradient” problem. It outputs the same value of its input if it is greater than 0 and outputs 0 in all other cases.<br>
<img class="img-responsive img-border img-full" src="img/dl9.jpg" alt=""><br>
<img class="img-responsive img-border img-full" src="img/dl10.jpg" alt=""><br>
The code for our class XORnet is given below for your reference:<br>
<img class="img-responsive img-border img-full" src="img/dl11.jpg" alt=""><br>
We then create our object of the XORnet class, which we’ll call MyNet.<br>
<img class="img-responsive img-border img-full" src="img/dl12.jpg" alt=""><br>
Now before we get to training our Neural network, we need to define the optimizer and the loss function.<br></p>
<p><b>Optimizer and Loss Function</b><br>
When we train our network, we need to calculate the deviation of the preliminary predictions from the actual values, using which we can train it by adjusting the weights in the nodes using an algorithm called backpropagation. The Backpropagation algorithm, in simple terms, is used to alter the weights of the nodes by feeding the error in prediction in the opposite direction of the feed-forward direction, hence the name Backpropagation. This error or deviation is calculated by the Loss Function. In this example, we use the Mean Squared Error Function to calculate the error. Optimization algorithms like Gradient Descent are used to train the network faster. These functions are used to find parameter values that will maximise or minimise the loss function. We use the Adam optimiser in this case, which is a powerful and popular learning rate optimization function, specifically used for training Neural Networks. We’ve chosen a learning rate of 0.001 for training our model.<br>
<img class="img-responsive img-border img-full" src="img/dl13.jpg" alt=""><br></p>
<p><b>Training the Model</b><br>
It’s finally time to train our Neural Network. We define a function train(), which takes a single parameter n_epochs. An epoch is one full cycle through the entire training dataset. So n_epochs refers to the number of training epochs we’ll use. In each epoch, we iterate through each value in the torchvision dataset we had defined earlier. The loss is calculated and is used to train the network at the end of each epoch via Backpropagation using the backward() function and the optimizer via the step() function.<br>
<img class="img-responsive img-border img-full" src="img/dl14.jpg" alt=""><br>
Now, we have the ever-important step of actually starting the training process by calling the train() function. Let’s train our model over 10000 epochs.<br></p>
<p><b>Visualizing our Training losses and Viewing our results:</b><br>
<img class="img-responsive img-border img-full" src="img/dl15.jpg" alt=""><br>
Using the plot() function in the matplotlib library, we can plot and visualize our losses in each step of the training process. As we can observe, there is significant flat lining of the error half way through the training function. You can try this with different values of the number epochs as well and compare the results.<br>
Now for the final result. Has the model made an accurate prediction?<br>
<img class="img-responsive img-border img-full" src="img/dl16.jpg" alt=""><br>
At first glance, it might seem like we haven’t really made that good of a prediction, but on closer observation, we can find that the ones made are really close to our expected values to the extent that they are approximately equal. So in other words, the model has made a successful prediction!<br></p>
<p><b>Conclusion:</b><br>
Congratulations on completing your first neural network using Pytorch! You can try improving the accuracy of your predictions further by modifying and playing with the different parameters used in the functions. This is but a small taste of what Pytorch is capable of. Neural networks using pytorch find a wide array of uses in image recognition, time-series analysis, predictive analysis, image generation and more! Hope this article has given you the curiosity and the drive to take your interest in pytorch forward.<br>
<h><b>In the words of Kirill Eremenko, “Enjoy Deep Learning!” .</b></h><br>
You can find the Jupyter Notebook with the full Python code here:<br>
<p><a href="https://github.com/sumitrj/Workshops/blob/master/PyTorch%20101.ipynb">click here for code</a></p>
<br>
<p><a href="https://www.hackerrank.com/sasukeuchiha0301">Click here to view my hackerrank page and please follow it</a></p>
<br>
<p> Stay tuned for more Articles on Gamedev(AI and ML), life as a gamedev and more from me, Thank you for reading, Cheers!</p>
<p style='color: green'>Credits - V Rahul</p>
<!-- LikeBtn.com BEGIN -->
<span class="likebtn-wrapper" data-theme="drop" data-ef_voting="bounce" data-identifier="item_504"></span>
<script>(function(d,e,s){if(d.getElementById("likebtn_wjs"))return;a=d.createElement(e);m=d.getElementsByTagName(e)[0];a.async=1;a.id="likebtn_wjs";a.src=s;m.parentNode.insertBefore(a, m)})(document,"script","//w.likebtn.com/js/w/widget.js");</script>
<!-- LikeBtn.com END -->
		  </div>
		  <div class="modal-footer">
			<button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
		  </div>
		</div>
	  </div>
	</div>

<!-- Modal 503 -->
	<div id="myModal503" class="modal fade" role="dialog">
	  <div class="modal-dialog">

		<!-- Modal content-->
		<div class="modal-content">
		  <div class="modal-header">
			<button type="button" class="close" data-dismiss="modal">&times;</button>
			<h4 class="modal-title">NEURAL NETWORKS AND DEEP LEARNING</h4>
		  </div>
		  <div class="modal-body">
<p>Deep learning is a hot topic these days. But what is it that makes it special and sets it apart from other aspects of machine learning? That is a deep question (pardon the pun). To even begin to answer it, we will need to learn the basics of neural networks.
Neural networks are the workhorses of deep learning. And while they may look like black boxes, deep down (sorry, I will stop the terrible puns) they are trying to accomplish the same thing as any other model — to make good predictions.
In this post, we will explore the ins and outs of a simple neural network. And by the end, hopefully you (and I) will have gained a deeper and more intuitive understanding of how neural networks do what they do.</p>
<br>
<h><b>The 30,000 Feet View</b></h>
<br>
<p>Let’s start with a really high level overview so we know what we are working with. Neural networks are multi-layer networks of neurons (the blue and magenta nodes in the chart below) that we use to classify things, make predictions, etc. Below is the diagram of a simple neural network with five inputs, 5 outputs, and two hidden layers of neurons.</p>
<br>
<img class="img-responsive img-border img-full" src="img/nn2.jpg" alt="">
<br>
<p>Starting from the left, we have:<br>
1-The input layer of our model in orange.<br>
2-Our first hidden layer of neurons in blue.<br>
3-Our second hidden layer of neurons in magenta.<br>
4-The output layer (a.k.a. the prediction) of our model in green.<br>
The arrows that connect the dots shows how all the neurons are interconnected and how data travels from the input layer all the way through to the output layer.
Later we will calculate step by step each output value. We will also watch how the neural network learns from its mistake using a process known as backpropagation.</p>
<br>
<h><b>Getting our Bearings</b></h>
<br>
<p>But first let’s get our bearings. What exactly is a neural network trying to do? Like any other model, it’s trying to make a good prediction. We have a set of inputs and a set of target values — and we are trying to get predictions that match those target values as closely as possible.
Forget for a second the more complicated looking picture of the neural network I drew above and focus on this simpler one below.</p>
<br>
<img class="img-responsive img-border img-full" src="img/nn3.jpg" alt="">
<br>
<p>This is a single feature logistic regression (we are giving the model only one X variable) expressed through a neural network (if you need a refresher on logistic regression, I wrote about that here). To see how they connect we can rewrite the logistic regression equation using our neural network color codes.</p>
<br>
<img class="img-responsive img-border img-full" src="img/nn4.jpg" alt="">
<br>
<p>Let’s examine each element:<br>
1-X (in orange) is our input, the lone feature that we give to our model in order to calculate a prediction.<br>
2-B1 (in turquoise, a.k.a. blue-green) is the estimated slope parameter of our logistic regression — B1 tells us by how much the Log_Odds change as X changes. Notice that B1 lives on the turquoise line, which connects the input X to the blue neuron in Hidden Layer 1.<br>
3-B0 (in blue) is the bias — very similar to the intercept term from regression. The key difference is that in neural networks, every neuron has its own bias term (while in regression, the model has a singular intercept term).<br>
4-The blue neuron also includes a sigmoid activation function (denoted by the curved line inside the blue circle). Remember the sigmoid function is what we use to go from log-odds to probability (do a control-f search for “sigmoid” in my previous post).<br>
5-And finally we get our predicted probability by applying the sigmoid function to the quantity (B1*X + B0).<br>
Not too bad right? So let’s recap. A super simple neural network consists of just the following components:<br>
*A connection (though in practice, there will generally be multiple connections, each with its own weight, going into a particular neuron), with a weight “living inside it”, that transforms your input (using B1) and gives it to the neuron.<br>
*A neuron that includes a bias term (B0) and an activation function (sigmoid in our case).<br>
And these two objects are the fundamental building blocks of the neural network. More complex neural networks are just models with more hidden layers and that means more neurons and more connections between neurons. And this more complex web of connections (and weights and biases) is what allows the neural network to “learn” the complicated relationships hidden in our data.</p>
<br>
<h><b>Let’s Add a Bit of Complexity Now</b></h>
<br>
<p>Now that we have our basic framework, let’s go back to our slightly more complicated neural network and see how it goes from input to output. Here it is again for reference:</p>
<br>
<img class="img-responsive img-border img-full" src="img/nn2.jpg" alt="">
<br>
<p>The first hidden layer consists of two neurons. So to connect all five inputs to the neurons in Hidden Layer 1, we need ten connections. The next image (below) shows just the connections between Input 1 and Hidden Layer 1.</p>
<br>
<img class="img-responsive img-border img-full" src="img/nn5.jpg" alt="">
<br>
<p>Note our notation for the weights that live in the connections — W1,1 denotes the weight that lives in the connection between Input 1 and Neuron 1 and W1,2 denotes the weight in the connection between Input 1 and Neuron 2. So the general notation that I will follow is Wa,b denotes the weight on the connection between Input a (or Neuron a) and Neuron b.
Now let’s calculate the outputs of each neuron in Hidden Layer 1 (known as the activations). We do so using the following formulas (W denotes weight, In denotes input).
Z1 = W1*In1 + W2*In2 + W3*In3 + W4*In4 + W5*In5 + Bias_Neuron1
Neuron 1 Activation = Sigmoid(Z1)
We can use matrix math to summarize this calculation (remember our notation rules — for example, W4,2 denotes the weight that lives in the connection between Input 4 and Neuron 2):</p>
<br>
<img class="img-responsive img-border img-full" src="img/nn6.jpg" alt="">
<br>
<p>For any layer of a neural network where the prior layer is m elements deep and the current layer is n elements deep, this generalizes to:
[W] @ [X] + [Bias] = [Z]
Where [W] is your n by m matrix of weights (the connections between the prior layer and the current layer), [X] is your m by 1 matrix of either starting inputs or activations from the prior layer, [Bias] is your n by 1 matrix of neuron biases, and [Z] is your n by 1 matrix of intermediate outputs. In the previous equation, I follow Python notation and use @ to denote matrix multiplication. Once we have [Z], we can apply the activation function (sigmoid in our case) to each element of [Z] and that gives us our neuron outputs (activations) for the current layer.
Finally before we move on, let’s visually map each of these elements back onto our neural network chart to tie it all up ([Bias] is embedded in the blue neurons).</p>
<br>
<img class="img-responsive img-border img-full" src="img/nn7.jpg" alt="">
<br>
<p>By repeatedly calculating [Z] and applying the activation function to it for each successive layer, we can move from input to output. This process is known as forward propagation. Now that we know how the outputs are calculated, it’s time to start evaluating the quality of the outputs and training our neural network.</p>
<br>
<h><b>Time for the Neural Network to Learn</b></h>
<br>
<p>This is going to be a long post so feel free to take a coffee break now. Still with me? Awesome! Now that we know how a neural network’s output values are calculated, it is time to train it.
The training process of a neural network, at a high level, is like that of many other data science models — define a cost function and use gradient descent optimization to minimize it.
First let’s think about what levers we can pull to minimize the cost function. In traditional linear or logistic regression we are searching for beta coefficients (B0, B1, B2, etc.) that minimize the cost function. For a neural network, we are doing the same thing but at a much larger and more complicated scale.
In traditional regression, we can change any particular beta in isolation without impacting the other beta coefficients. So by applying small isolated shocks to each beta coefficient and measuring its impact on the cost function, it is relatively straightforward to figure out in which direction we need to move to reduce and eventually minimize the cost function.</p>
<br>
<img class="img-responsive img-border img-full" src="img/nn8.jpg" alt="">
<br>
<p>In a neural network, changing the weight of any one connection (or the bias of a neuron) has a reverberating effect across all the other neurons and their activations in the subsequent layers.
That’s because each neuron in a neural network is like its own little model. For example, if we wanted a five feature logistic regression, we could express it through a neural network, like the one on the left, using just a singular neuron!
So each hidden layer of a neural network is basically a stack of models (each individual neuron in the layer acts like its own model) whose outputs feed into even more models further downstream (each successive hidden layer of the neural network holds yet more neurons).</p>
<br>
<h><b>The Cost Function</b></h>
<br>
<p>So given all this complexity, what can we do? It’s actually not that bad. Let’s take it step by step. First, let me clearly state our objective. Given a set of training inputs (our features) and outcomes (the target we are trying to predict):
We want to find the set of weights (remember that each connecting line between any two elements in a neural network houses a weight) and biases (each neuron houses a bias) that minimize our cost function — where the cost function is an approximation of how wrong our predictions are relative to the target outcome.
For training our neural network, we will use Mean Squared Error (MSE) as the cost function:
MSE = Sum [ ( Prediction - Actual )² ] * (1 / num_observations)
The MSE of a model tell us on average how wrong we are but with a twist — by squaring the errors of our predictions before averaging them, we punish predictions that are way off much more severely than ones that are just slightly off. The cost functions of linear regression and logistic regression operate in a very similar manner.
OK cool, we have a cost function to minimize. Time to fire up gradient descent right?
Not so fast — to use gradient descent, we need to know the gradient of our cost function, the vector that points in the direction of greatest steepness (we want to repeatedly take steps in the opposite direction of the gradient to eventually arrive at the minimum).
Except in a neural network we have so many changeable weights and biases that are all interconnected. How will we calculate the gradient of all of that? In the next section, we will see how backpropagation helps us deal with this problem.</p>
<br>
<h><b>Quick Review of Gradient Descent</b></h>
<br>
<p>The gradient of a function is the vector whose elements are its partial derivatives with respect to each parameter. For example, if we were trying to minimize a cost function, C(B0, B1), with just two changeable parameters, B0 and B1, the gradient would be:
Gradient of C(B0, B1) = [ [dC/dB0], [dC/dB1] ] </p>
<br>
<img class="img-responsive img-border img-full" src="img/nn9.jpg" alt="">
<br>
<p>So each element of the gradient tells us how the cost function would change if we applied a small change to that particular parameter — so we know what to tweak and by how much. To summarize, we can march towards the minimum by following these steps:</p>
<br>
<p>Compute the gradient of our “current location” (calculate the gradient using our current parameter values).
Modify each parameter by an amount proportional to its gradient element and in the opposite direction of its gradient element. For example, if the partial derivative of our cost function with respect to B0 is positive but tiny and the partial derivative with respect to B1 is negative and large, then we want to decrease B0 by a tiny amount and increase B1 by a large amount to lower our cost function.
Recompute the gradient using our new tweaked parameter values and repeat the previous steps until we arrive at the minimum.</p>
<br>
<h><b>Backpropagation</b></h>
<br>
<p>I will defer to this great textbook (online and free!) for the detailed math (if you want to understand neural networks more deeply, definitely check it out). Instead we will do our best to build an intuitive understanding of how and why backpropagation works.
Remember that forward propagation is the process of moving forward through the neural network (from inputs to the ultimate output or prediction). Backpropagation is the reverse. Except instead of signal, we are moving error backwards through our model.
Some simple visualizations helped a lot when I was trying to understand the backpropagation process. Below is my mental picture of a simple neural network as it forward propagates from input to output. The process can be summarized by the following steps:
Inputs are fed into the blue layer of neurons and modified by the weights, bias, and sigmoid in each neuron to get the activations. For example: Activation_1 = Sigmoid( Bias_1 + W1*Input_1 )
Activation 1 and Activation 2, which come out of the blue layer are fed into the magenta neuron, which uses them to produce the final output activation.
And the objective of forward propagation is to calculate the activations at each neuron for each successive hidden layer until we arrive at the output.</p>
<br>
<img class="img-responsive img-border img-full" src="img/nn10.jpg" alt="">
<br>
<p>Now let’s just reverse it. If you follow the red arrows (in the picture below), you will notice that we are now starting at the output of the magenta neuron. That is our output activation, which we use to make our prediction, and the ultimate source of error in our model. We then move this error backwards through our model via the same weights and connections that we use for forward propagating our signal (so instead of Activation 1, now we have Error1 — the error attributable to the top blue neuron).
Remember we said that the goal of forward propagation is to calculate neuron activations layer by layer until we get to the output? We can now state the objective of backpropagation in a similar manner:
We want to calculate the error attributable to each neuron (I will just refer to this error quantity as the neuron’s error because saying “attributable” again and again is no fun) starting from the layer closest to the output all the way back to the starting layer of our model.</p>
<br>
<img class="img-responsive img-border img-full" src="img/nn11.jpg" alt="">
<br>
<p>So why do we care about the error for each neuron? Remember that the two building blocks of a neural network are the connections that pass signals into a particular neuron (with a weight living in each connection) and the neuron itself (with a bias). These weights and biases across the entire network are also the dials that we tweak to change the predictions made by the model.
This part is really important:
The magnitude of the error of a specific neuron (relative to the errors of all the other neurons) is directly proportional to the impact of that neuron’s output (a.k.a. activation) on our cost function.
So the error of each neuron is a proxy for the partial derivative of the cost function with respect to that neuron’s inputs. This makes intuitive sense — if a particular neuron has a much larger error than all the other ones, then tweaking the weights and bias of our offending neuron will have a greater impact on our model’s total error than fiddling with any of the other neurons.
And the partial derivatives with respect to each weight and bias are the individual elements that compose the gradient vector of our cost function. So basically backpropagation allows us to calculate the error attributable to each neuron and that in turn allows us to calculate the partial derivatives and ultimately the gradient so that we can utilize gradient descent. Hurray!</p>
<br>
<h><b>An Analogy that Helps — The Blame Game</b></h>
<br>
<p>That’s a lot to digest so hopefully this analogy will help. Almost everyone has had a terrible colleague at some point in his or her life — someone who would always play the blame game and throw coworkers or subordinates under the bus when things went wrong.</p>
<br>
<img class="img-responsive img-border img-full" src="img/nn12.jpg" alt="">
<br>
<p>Well neurons, via backpropagation, are masters of the blame game. When the error gets backpropagated to a particular neuron, that neuron will quickly and efficiently point the finger at the upstream colleague (or colleagues) who is most at fault for causing the error (i.e. layer 4 neurons would point the finger at layer 3 neurons, layer 3 neurons at layer 2 neurons, and so forth).</p>
<br>
<p>And how does each neuron know who to blame, as the neurons cannot directly observe the errors of other neurons? They just look at who sent them the most signal in terms of the highest and most frequent activations. Just like in real life, the lazy ones that play it safe (low and infrequent activations) skate by blame free while the neurons that do the most work get blamed and have their weights and biases modified. Cynical yes but also very effective for getting us to the optimal set of weights and biases that minimize our cost function. To the left is a visual of how the neurons throw each other under the bus.
And that in a nutshell is the intuition behind the backpropagation process. In my opinion, these are the three key takeaways for backpropagation:<br>
1-It is the process of shifting the error backwards layer by layer and attributing the correct amount of error to each neuron in the neural network.<br>
2-The error attributable to a particular neuron is a good approximation for how changing that neuron’s weights (from the connections leading into the neuron) and bias will affect the cost function.<br>
3-When looking backwards, the more active neurons (the non-lazy ones) are the ones that get blamed and tweaked by the backpropagation process.</p>
<br>
<h><b>Tying it All Together</b></h>
<br>
<p>If you have read all the way here, then you have my gratitude and admiration (for your persistence).
We started with a question, “What makes deep learning special?” I will attempt to answer that now (mainly from the perspective of basic neural networks and not their more advanced cousins like CNNs, RNNs, etc.). In my humble opinion, the following aspects make neural networks special:<br>
*Each neuron is its own miniature model with its own bias and set of incoming features and weights.<br>
*Each individual model/neuron feeds into numerous other individual neurons across all the hidden layers of the model. So we end up with models plugged into other models in a way where the sum is greater than its parts. This allows neural networks to fit all the nooks and crannies of our data including the nonlinear parts (but beware overfitting — and definitely consider regularization to protect your model from underperforming when confronted with new and out of sample data).<br>
*The versatility of the many interconnected models approach and the ability of the backpropagation process to efficiently and optimally set the weights and biases of each model lets the neural network to robustly “learn” from data in ways that many other algorithms cannot.</p>
<br>
<p><a href="https://www.hackerrank.com/sasukeuchiha0301">Click here to view my hackerrank page and please follow it</a></p>
<br>
<p> Stay tuned for more Articles on Gamedev(AI and ML), life as a gamedev and more from me, Thank you for reading, Cheers!</p>
<p style='color: green'>Credits - V Rahul</p>
<!-- LikeBtn.com BEGIN -->
<span class="likebtn-wrapper" data-theme="drop" data-ef_voting="bounce" data-identifier="item_503"></span>
<script>(function(d,e,s){if(d.getElementById("likebtn_wjs"))return;a=d.createElement(e);m=d.getElementsByTagName(e)[0];a.async=1;a.id="likebtn_wjs";a.src=s;m.parentNode.insertBefore(a, m)})(document,"script","//w.likebtn.com/js/w/widget.js");</script>
<!-- LikeBtn.com END -->
		  </div>
		  <div class="modal-footer">
			<button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
		  </div>
		</div>
	  </div>
	</div>
	


	<!-- Modal 36 -->
    <div id="myModal36" class="modal fade" role="dialog">
	  <div class="modal-dialog">

		<!-- Modal content-->
		<div class="modal-content">
		  <div class="modal-header">
			<button type="button" class="close" data-dismiss="modal">&times;</button>
			<h4 class="modal-title">Machine Learning</h4>
		  </div>
		  <div class="modal-body">
	
<p><i>" Computer was invented to complete the job easier and effecient but how much ever the features are made uncomplicated the users want to complete the job as simple as one touch.... So to achieve this Machine Learning is the only way "</p>
<h2>What is Machine Learning ?</h2>
<p>Machine Learning is getting computers to program themselves. If programming is automation, then machine learning is automating the process of automation.
In simple words writing software is the bottleneck, we don’t have enough good developers. Let the data do the work instead of people. Machine learning is the way to make programming scalable.<br>
The basic difference is :<br>
>Traditional Programming: Data and program is run on the computer to produce the output.
>Machine Learning: Data and output is run on the computer to create a program. This program can be used in traditional programming.</p>
<h2>Basic elements in Machine learning</h2>
<p>Every machine learning algorithm has three components:<br>
<b>Representation:</b> how to represent knowledge. Examples include decision trees, sets of rules, instances, graphical models, neural networks, support vector machines, model ensembles and others.<br>
<b>Evaluation:</b> the way to evaluate candidate programs (hypotheses). Examples include accuracy, prediction and recall, squared error, likelihood, posterior probability, cost, margin, entropy k-L divergence and others.<br>
<b>Optimization:</b> the way candidate programs are generated known as the search process. For example combinatorial optimization, convex optimization, constrained optimization.<br></p>
<h2>Applications of Machine Learning</h2>
<p><b>Web search:</b> ranking page based on what you are most likely to click on.<br>
<b>Computational biology:</b> rational design drugs in the computer based on past experiments.<br>
<b>Finance:</b> decide who to send what credit card offers to. Evaluation of risk on credit offers. How to decide where to invest money.<br>
<b>E-commerce:</b>  Predicting customer churn. Whether or not a transaction is fraudulent.<br>
<b>Space exploration:</b> space probes and radio astronomy.<br>
<b>Robotics:</b> how to handle uncertainty in new environments. Autonomous. Self-driving car.<br>
<b>Information extraction:</b> Ask questions over databases across the web.<br>
<b>Social networks:</b> Data on relationships and preferences. Machine learning to extract value from data.<br>
<b>Debugging:</b> Use in computer science problems like debugging. Labor intensive process. Could suggest where the bug could be.<br></p>
<h2>Types of learning</h2>
<p>There are four types of machine learning:<br>
<b>Supervised learning:</b> (also called inductive learning) Training data includes desired outputs.  This is spam this is not, learning is supervised.<br>
<b>Unsupervised learning:</b> Training data does not include desired outputs. Example is clustering. It is hard to tell what is good learning and what is not.<br>
<b>Semi-supervised learning:</b> Training data includes a few desired outputs.<br>
<b>Reinforcement learning:</b> Rewards from a sequence of actions. AI types like it, it is the most ambitious type of learning.<br>
Supervised learning is the most mature, the most studied and the type of learning used by most machine learning algorithms. Learning with supervision is much easier than learning without supervision.</p>
<b>" Machine learning is a subset of Artificial Intelligence but without Machine Learning Artifical Intelligence makes the bird without wings "</b>
<p style='color: green'><h5>credits - vignesh g</h5></p></i>
<!-- LikeBtn.com BEGIN -->
<span class="likebtn-wrapper" data-theme="drop" data-ef_voting="bounce" data-identifier="item_10"></span>
<script>(function(d,e,s){if(d.getElementById("likebtn_wjs"))return;a=d.createElement(e);m=d.getElementsByTagName(e)[0];a.async=1;a.id="likebtn_wjs";a.src=s;m.parentNode.insertBefore(a, m)})(document,"script","//w.likebtn.com/js/w/widget.js");</script>
<!-- LikeBtn.com END -->
		 
          </div>
		  <div class="modal-footer">
			<button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
		  </div>
		</div>
	  </div>
	</div>

	
	
	
	
	
	
	
	
	
	<footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <p>Copyright &copy; Engineers Eden june 2020</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>


				

